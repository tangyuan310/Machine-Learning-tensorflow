{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 4743\n",
      "N = 4743\n",
      "N_test = 1701\n",
      "M = 8787\n",
      "N = 4743\n",
      "N_test = 1701\n",
      "\n",
      "when donald trump goes low   register to vote \n",
      "[2179, 3482, 5531, 1065, 6065, 7920, 1742, 1085]\n",
      "\n",
      "its nationalvoterregistrationday  celebrate by registering to vote â†’ \n",
      "[7438, 1888, 4778, 6346, 6413, 1742, 1085, 795]\n",
      "\n",
      "N_train = 3794\n",
      "N_valid = 948\n"
     ]
    }
   ],
   "source": [
    "## read text data from file\n",
    "\n",
    "import csv\n",
    "import string\n",
    "\n",
    "labelList = []\n",
    "tweetList = []\n",
    "tweetList_test = []\n",
    "\n",
    "with open('train.csv', encoding='utf-8') as trainfile:\n",
    "    reader = csv.reader(trainfile)\n",
    "    for row in reader:\n",
    "        if row[0] == 'HillaryClinton':\n",
    "            labelList.append(1)\n",
    "        elif row[0] == 'realDonaldTrump':\n",
    "            labelList.append(0)\n",
    "            \n",
    "        if row[1] != 'tweet':\n",
    "            tweetList.append(row[1])\n",
    "            \n",
    "with open('test.csv', encoding='utf-8') as testfile:\n",
    "    reader = csv.reader(testfile)\n",
    "    for row in reader: \n",
    "        if row[1] != 'tweet':\n",
    "            tweetList_test.append(row[1])\n",
    "\n",
    "N = len(tweetList)\n",
    "print('N = %d' % len(tweetList))\n",
    "print('N = %d' % len(labelList))\n",
    "\n",
    "N_test = len(tweetList_test)\n",
    "print('N_test = %d' % len(tweetList_test))\n",
    "\n",
    "##############################################################\n",
    "\n",
    "## corpus\n",
    "import string\n",
    "\n",
    "def refineWordsList(tweetList):\n",
    "    ret = []\n",
    "    for tweet in tweetList:\n",
    "        step1 = tweet.split('http',1)[0]\n",
    "        step2 = step1.lower()\n",
    "        step3 = step2.translate(str.maketrans('.',' '))\n",
    "        step4 = step3.translate(str.maketrans('','',string.punctuation))\n",
    "        ret.append(step4)\n",
    "    return ret\n",
    "\n",
    "tweetList = refineWordsList(tweetList)\n",
    "tweetList_test = refineWordsList(tweetList_test)\n",
    "\n",
    "corpus = set()\n",
    "\n",
    "for tweet in tweetList:\n",
    "    words = tweet.split();\n",
    "    corpus.update(words)\n",
    "\n",
    "corpus = list(corpus)\n",
    "M = len(corpus)\n",
    "print('M = %d' % M)\n",
    "\n",
    "##############################################################\n",
    "\n",
    "## Label Encoding\n",
    "\n",
    "hashEncoder = {}\n",
    "i = 0\n",
    "for word in corpus:\n",
    "    hashEncoder[word] = i\n",
    "    i += 1\n",
    "\n",
    "# train\n",
    "tweetList_encoded = []\n",
    "for tweet in tweetList:\n",
    "    tweet_encoded = []\n",
    "    for word in tweet.split():\n",
    "        word_encoded = hashEncoder[word]\n",
    "        tweet_encoded.append(word_encoded)\n",
    "    tweetList_encoded.append(tweet_encoded)\n",
    "    \n",
    "# test\n",
    "tweetList_test_encoded = []\n",
    "for tweet in tweetList_test:\n",
    "    tweet_encoded = []\n",
    "    for word in tweet.split():\n",
    "        if hashEncoder.get(word) != None :\n",
    "            word_encoded = hashEncoder[word]\n",
    "            tweet_encoded.append(word_encoded)\n",
    "    tweetList_test_encoded.append(tweet_encoded)\n",
    "\n",
    "def decodeTweet(tweet_encoded):\n",
    "    decodeList = []\n",
    "    for word_encoded in tweetList_encoded[5]:\n",
    "        decodeList.append(list(hashEncoder.keys())[list(hashEncoder.values()).index(word_encoded)])\n",
    "    return decodeList\n",
    "    \n",
    "print('N = %d' % len(tweetList_encoded))\n",
    "print('N_test = %d' % len(tweetList_test_encoded))\n",
    "print()\n",
    "print(tweetList[5])\n",
    "print(tweetList_encoded[5])\n",
    "#print(decodeTweet(tweetList_encoded[5]))\n",
    "print()\n",
    "print(tweetList_test[5])\n",
    "print(tweetList_test_encoded[5])\n",
    "print()\n",
    "\n",
    "##############################################################\n",
    "\n",
    "N_train = (int)(0.8 * N)\n",
    "N_valid = N - (int)(0.8 * N) - 1\n",
    "\n",
    "tweetList_encoded_train = [tweetList_encoded[x] for x in range (N_train)]\n",
    "tweetList_encoded_valid = [tweetList_encoded[x] for x in range (N_train + 1, N)]\n",
    "\n",
    "labelList_train = [labelList[x] for x in range (N_train)]\n",
    "labelList_valid = [labelList[x] for x in range (N_train + 1, N)]\n",
    "\n",
    "print('N_train = %d' % N_train)\n",
    "print('N_valid = %d' % N_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceData(object):\n",
    "    \n",
    "    def __init__(self, n_samples=1000, max_seq_len = 31, x_list=[], t_list=[]):\n",
    "        \n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        self.seqlen = []\n",
    "        \n",
    "        for i in range(n_samples):\n",
    "            len1 = len(x_list[i])\n",
    "            self.seqlen.append(len1)\n",
    "            s = [[x_list[i][j]] for j in range(len1)]\n",
    "            s += [[0] for j in range(max_seq_len - len1 )]\n",
    "            self.data.append(s)\n",
    "            self.labels.append([t_list[i], 1 - t_list[i]])\n",
    "            \n",
    "        self.batch_id = 0\n",
    "\n",
    "    def next(self, batch_size):\n",
    "\n",
    "        if self.batch_id == len(self.data):\n",
    "            self.batch_id = 0\n",
    "        batch_data = (self.data[self.batch_id:min(self.batch_id +\n",
    "                                                  batch_size, len(self.data))])\n",
    "        batch_labels = (self.labels[self.batch_id:min(self.batch_id +\n",
    "                                                  batch_size, len(self.data))])\n",
    "        batch_seqlen = (self.seqlen[self.batch_id:min(self.batch_id +\n",
    "                                                  batch_size, len(self.data))])\n",
    "        self.batch_id = min(self.batch_id + batch_size, len(self.data))\n",
    "        return batch_data, batch_labels, batch_seqlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========\n",
    "#   MODEL\n",
    "# ==========\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "training_steps = 1000\n",
    "batch_size = 128\n",
    "display_step = 100\n",
    "\n",
    "# Network Parameters\n",
    "seq_max_len = 31 # Sequence max length\n",
    "n_hidden = 31 #todo?? hidden layer num of features\n",
    "n_classes = 2 # linear sequence or notstate_size = 1#20\n",
    "\n",
    "state_size = 1#20\n",
    "\n",
    "trainset = SequenceData(n_samples = N_train,\n",
    "                           max_seq_len = seq_max_len,\n",
    "                           x_list = tweetList_encoded_train,\n",
    "                           t_list = labelList_train)\n",
    "\n",
    "testset = SequenceData(n_samples = N_valid,\n",
    "                          max_seq_len = seq_max_len,\n",
    "                          x_list = tweetList_encoded_valid,\n",
    "                          t_list = labelList_valid)\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(\"float\", [None, seq_max_len, state_size])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "# A placeholder for indicating each sequence length\n",
    "seqlen = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "# Define weights\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamicRNN(x, seqlen, weights, biases):\n",
    "\n",
    "    # Prepare data shape to match `rnn` function requirements\n",
    "    # Current data input shape: (batch_size, n_steps, n_input)\n",
    "    # Required shape: 'n_steps' tensors list of shape (batch_size, n_input)\n",
    "    \n",
    "    # Unstack to get a list of 'n_steps' tensors of shape (batch_size, n_input)\n",
    "    x = tf.unstack(x, seq_max_len, 1)\n",
    "\n",
    "    # Define a lstm cell with tensorflow\n",
    "    lstm_cell = tf.contrib.rnn.BasicLSTMCell(n_hidden)\n",
    "\n",
    "    # Get lstm cell output, providing 'sequence_length' will perform dynamic\n",
    "    # calculation.\n",
    "    outputs, states = tf.contrib.rnn.static_rnn(lstm_cell, x, dtype=tf.float32,\n",
    "                                sequence_length=seqlen)\n",
    "\n",
    "    # When performing dynamic calculation, we must retrieve the last\n",
    "    # dynamically computed output, i.e., if a sequence length is 10, we need\n",
    "    # to retrieve the 10th output.\n",
    "    # However TensorFlow doesn't support advanced indexing yet, so we build\n",
    "    # a custom op that for each sample in batch size, get its length and\n",
    "    # get the corresponding relevant output.\n",
    "\n",
    "    # 'outputs' is a list of output at every timestep, we pack them in a Tensor\n",
    "    # and change back dimension to [batch_size, n_step, n_input]\n",
    "    outputs = tf.stack(outputs)\n",
    "    outputs = tf.transpose(outputs, [1, 0, 2])\n",
    "\n",
    "    # Hack to build the indexing and retrieve the right output.\n",
    "    batch_size = tf.shape(outputs)[0]\n",
    "    # Start indices for each sample\n",
    "    index = tf.range(0, batch_size) * seq_max_len + (seqlen - 1)\n",
    "    # Indexing\n",
    "    outputs = tf.gather(tf.reshape(outputs, [-1, n_hidden]), index)\n",
    "\n",
    "    # Linear activation, using outputs computed above\n",
    "    return tf.matmul(outputs, weights['out']) + biases['out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:96: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "pred = dynamicRNN(x, seqlen, weights, biases)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1, Minibatch Loss= 1.139106, Training Accuracy= 0.67969\n",
      "Step 100, Minibatch Loss= 0.686262, Training Accuracy= 0.60938\n",
      "Step 200, Minibatch Loss= 0.629039, Training Accuracy= 0.68750\n",
      "Step 300, Minibatch Loss= 0.877589, Training Accuracy= 0.00000\n",
      "Step 400, Minibatch Loss= 0.677515, Training Accuracy= 0.60156\n",
      "Step 500, Minibatch Loss= 0.626252, Training Accuracy= 0.68750\n",
      "Step 600, Minibatch Loss= 0.867148, Training Accuracy= 0.00000\n",
      "Step 700, Minibatch Loss= 0.677670, Training Accuracy= 0.60156\n",
      "Step 800, Minibatch Loss= 0.625474, Training Accuracy= 0.68750\n",
      "Step 900, Minibatch Loss= 0.866599, Training Accuracy= 0.00000\n",
      "Step 1000, Minibatch Loss= 0.677731, Training Accuracy= 0.60156\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.0021097\n"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "\n",
    "    for step in range(1, training_steps+1):\n",
    "        batch_x, batch_y, batch_seqlen = trainset.next(batch_size)\n",
    "        # Run optimization op (backprop)\n",
    "        sess.run(optimizer, feed_dict={x: batch_x, y: batch_y,\n",
    "                                       seqlen: batch_seqlen})\n",
    "        if step % display_step == 0 or step == 1:\n",
    "            # Calculate batch accuracy & loss\n",
    "            acc, loss = sess.run([accuracy, cost], feed_dict={x: batch_x, y: batch_y,\n",
    "                                                seqlen: batch_seqlen})\n",
    "            print(\"Step \" + str(step) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.5f}\".format(acc))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Calculate accuracy\n",
    "    test_data = testset.data\n",
    "    test_label = testset.labels\n",
    "    test_seqlen = testset.seqlen\n",
    "    print(\"Testing Accuracy:\", \\\n",
    "        sess.run(accuracy, feed_dict={x: test_data, y: test_label,\n",
    "                                      seqlen: test_seqlen}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
