{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 4743\n",
      "N = 4743\n",
      "N_test = 1701\n",
      "M = 8787\n",
      "N = 4743\n",
      "N_test = 1701\n",
      "\n",
      "when donald trump goes low   register to vote \n",
      "[6480, 1431, 6169, 400, 3970, 2288, 1482, 7957]\n",
      "\n",
      "its nationalvoterregistrationday  celebrate by registering to vote â†’ \n",
      "[7162, 3020, 6345, 1116, 8559, 1482, 7957, 1219]\n",
      "\n",
      "N_train = 4268\n",
      "N_valid = 474\n"
     ]
    }
   ],
   "source": [
    "## read text data from file\n",
    "\n",
    "import csv\n",
    "import string\n",
    "\n",
    "labelList = []\n",
    "tweetList = []\n",
    "tweetList_test = []\n",
    "\n",
    "with open('train.csv', encoding='utf-8') as trainfile:\n",
    "    reader = csv.reader(trainfile)\n",
    "    for row in reader:\n",
    "        if row[0] == 'HillaryClinton':\n",
    "            labelList.append(1)\n",
    "        elif row[0] == 'realDonaldTrump':\n",
    "            labelList.append(0)\n",
    "            \n",
    "        if row[1] != 'tweet':\n",
    "            tweetList.append(row[1])\n",
    "            \n",
    "with open('test.csv', encoding='utf-8') as testfile:\n",
    "    reader = csv.reader(testfile)\n",
    "    for row in reader: \n",
    "        if row[1] != 'tweet':\n",
    "            tweetList_test.append(row[1])\n",
    "\n",
    "N = len(tweetList)\n",
    "print('N = %d' % len(tweetList))\n",
    "print('N = %d' % len(labelList))\n",
    "\n",
    "N_test = len(tweetList_test)\n",
    "print('N_test = %d' % len(tweetList_test))\n",
    "\n",
    "##############################################################\n",
    "\n",
    "## corpus\n",
    "import string\n",
    "\n",
    "def refineWordsList(tweetList):\n",
    "    ret = []\n",
    "    for tweet in tweetList:\n",
    "        step1 = tweet.split('http',1)[0]\n",
    "        step2 = step1.lower()\n",
    "        step3 = step2.translate(str.maketrans('.',' '))\n",
    "        step4 = step3.translate(str.maketrans('','',string.punctuation))\n",
    "        ret.append(step4)\n",
    "    return ret\n",
    "\n",
    "tweetList = refineWordsList(tweetList)\n",
    "tweetList_test = refineWordsList(tweetList_test)\n",
    "\n",
    "corpus = set()\n",
    "\n",
    "for tweet in tweetList:\n",
    "    words = tweet.split();\n",
    "    corpus.update(words)\n",
    "\n",
    "corpus = list(corpus)\n",
    "M = len(corpus)\n",
    "print('M = %d' % M)\n",
    "\n",
    "##############################################################\n",
    "\n",
    "## Label Encoding\n",
    "\n",
    "hashEncoder = {}\n",
    "i = 0\n",
    "for word in corpus:\n",
    "    hashEncoder[word] = i\n",
    "    i += 1\n",
    "\n",
    "# train\n",
    "tweetList_encoded = []\n",
    "for tweet in tweetList:\n",
    "    tweet_encoded = []\n",
    "    for word in tweet.split():\n",
    "        word_encoded = hashEncoder[word]\n",
    "        tweet_encoded.append(word_encoded)\n",
    "    tweetList_encoded.append(tweet_encoded)\n",
    "    \n",
    "# test\n",
    "tweetList_test_encoded = []\n",
    "for tweet in tweetList_test:\n",
    "    tweet_encoded = []\n",
    "    for word in tweet.split():\n",
    "        if hashEncoder.get(word) != None :\n",
    "            word_encoded = hashEncoder[word]\n",
    "            tweet_encoded.append(word_encoded)\n",
    "    tweetList_test_encoded.append(tweet_encoded)\n",
    "\n",
    "def decodeTweet(tweet_encoded):\n",
    "    decodeList = []\n",
    "    for word_encoded in tweetList_encoded[5]:\n",
    "        decodeList.append(list(hashEncoder.keys())[list(hashEncoder.values()).index(word_encoded)])\n",
    "    return decodeList\n",
    "    \n",
    "print('N = %d' % len(tweetList_encoded))\n",
    "print('N_test = %d' % len(tweetList_test_encoded))\n",
    "print()\n",
    "print(tweetList[5])\n",
    "print(tweetList_encoded[5])\n",
    "#print(decodeTweet(tweetList_encoded[5]))\n",
    "print()\n",
    "print(tweetList_test[5])\n",
    "print(tweetList_test_encoded[5])\n",
    "print()\n",
    "\n",
    "##############################################################\n",
    "\n",
    "N_train = (int)(0.9 * N)\n",
    "N_valid = N - (int)(0.9 * N) - 1\n",
    "\n",
    "tweetList_encoded_train = [tweetList_encoded[x] for x in range (N_train)]\n",
    "tweetList_encoded_valid = [tweetList_encoded[x] for x in range (N_train + 1, N)]\n",
    "\n",
    "labelList_train = [labelList[x] for x in range (N_train)]\n",
    "labelList_valid = [labelList[x] for x in range (N_train + 1, N)]\n",
    "\n",
    "print('N_train = %d' % N_train)\n",
    "print('N_valid = %d' % N_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceData(object):\n",
    "    \n",
    "    def __init__(self, n_samples=1000, max_seq_len = 31, x_list=[], t_list=[]):\n",
    "        \n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        self.seqlen = []\n",
    "        \n",
    "        for i in range(n_samples):\n",
    "            len1 = len(x_list[i])\n",
    "            self.seqlen.append(len1)\n",
    "            s = [x_list[i][j] for j in range(len1)]\n",
    "            s += [0 for j in range(max_seq_len - len1)]\n",
    "            self.data.append(s)\n",
    "            \n",
    "            if len(t_list) != 0:\n",
    "                self.labels.append([t_list[i], 1 - t_list[i]])\n",
    "            else:\n",
    "                self.labels.append([1, 0])\n",
    "            \n",
    "        self.batch_id = 0\n",
    "\n",
    "    def next(self, batch_size):\n",
    "\n",
    "        if self.batch_id == len(self.data):\n",
    "            self.batch_id = 0\n",
    "            \n",
    "        batch_data = (self.data[self.batch_id:min(self.batch_id + batch_size, len(self.data))])\n",
    "        \n",
    "        batch_labels = (self.labels[self.batch_id:min(self.batch_id + batch_size, len(self.data))])\n",
    "        \n",
    "        batch_seqlen = (self.seqlen[self.batch_id:min(self.batch_id + batch_size, len(self.data))])\n",
    "        \n",
    "        self.batch_id = min(self.batch_id + batch_size, len(self.data))\n",
    "        \n",
    "        return batch_data, batch_labels, batch_seqlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========\n",
    "#   MODEL\n",
    "# ==========\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "training_steps = 30000\n",
    "batch_size = 128\n",
    "display_step = 1000\n",
    "\n",
    "# Network Parameters\n",
    "seq_max_len = 32 # Sequence max length\n",
    "word_features = 25\n",
    "n_hidden_1 = 50 # 1st layer number of neurons\n",
    "n_hidden_2 = 50 # 2nd layer number of neurons\n",
    "n_classes = 2 # linear sequence or notstate_size = 1#20\n",
    "\n",
    "#trainset = SequenceData(n_samples = N_train,\n",
    "#                           max_seq_len = seq_max_len,\n",
    "#                           x_list = tweetList_encoded_train,\n",
    "#                           t_list = labelList_train)\n",
    "\n",
    "#testset = SequenceData(n_samples = N_valid,\n",
    "#                          max_seq_len = seq_max_len,\n",
    "#                          x_list = tweetList_encoded_valid,\n",
    "#                          t_list = labelList_valid)\n",
    "\n",
    "trainset = SequenceData(n_samples = N,\n",
    "                           max_seq_len = seq_max_len,\n",
    "                           x_list = tweetList_encoded,\n",
    "                           t_list = labelList)\n",
    "\n",
    "testset = SequenceData(n_samples = N_test,\n",
    "                       max_seq_len = seq_max_len,\n",
    "                       x_list = tweetList_test_encoded,\n",
    "                       t_list = [])\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(\"int32\", [None, seq_max_len])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# A placeholder for indicating each sequence length\n",
    "seqlen = tf.placeholder(tf.int32, [None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([word_features, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "embeddings = tf.Variable(tf.random_uniform([M, 20],-1.0,1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_net(x, weights, biases):\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamicRNN(x, seqlen, weights, biases, embeddings):\n",
    "    # Prepare data shape to match `rnn` function requirements\n",
    "    # Current data input shape: (batch_size, n_steps, n_input)\n",
    "    # Required shape: 'n_steps' tensors list of shape (batch_size, n_input)\n",
    "    x = tf.nn.embedding_lookup(embeddings, x)\n",
    "    # Unstack to get a list of 'n_steps' tensors of shape (batch_size, n_input)\n",
    "    x = tf.unstack(x, seq_max_len, 1)\n",
    "    \n",
    "    # Define a lstm cell with tensorflow\n",
    "    lstm_cell = tf.contrib.rnn.BasicLSTMCell(word_features)\n",
    "\n",
    "    # Get lstm cell output, providing 'sequence_length' will perform dynamic\n",
    "    # calculation.\n",
    "    outputs, states = tf.contrib.rnn.static_rnn(lstm_cell\n",
    "                                                ,x\n",
    "                                                ,dtype=tf.float32\n",
    "                                                ,sequence_length=seqlen)\n",
    "\n",
    "    # When performing dynamic calculation, we must retrieve the last\n",
    "    # dynamically computed output, i.e., if a sequence length is 10, we need\n",
    "    # to retrieve the 10th output.\n",
    "    # However TensorFlow doesn't support advanced indexing yet, so we build\n",
    "    # a custom op that for each sample in batch size, get its length and\n",
    "    # get the corresponding relevant output.\n",
    "\n",
    "    # 'outputs' is a list of output at every timestep, we pack them in a Tensor\n",
    "    # and change back dimension to [batch_size, n_step, n_input]\n",
    "    outputs = tf.stack(outputs)\n",
    "    outputs = tf.transpose(outputs, [1, 0, 2])\n",
    "\n",
    "    # Hack to build the indexing and retrieve the right output.\n",
    "    batch_size = tf.shape(outputs)[0]\n",
    "    # Start indices for each sample\n",
    "    index = tf.range(batch_size) * seq_max_len + (seqlen - 1)\n",
    "    # Indexing\n",
    "    outputs = tf.gather(tf.reshape(outputs, [-1, word_features]), index)\n",
    "\n",
    "    return neural_net(outputs, weights, biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:96: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "pred = dynamicRNN(x, seqlen, weights, biases, embeddings)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Evaluate model\n",
    "class_pred = tf.argmax(pred, axis=1)\n",
    "prob_pred = tf.nn.softmax(pred)\n",
    "\n",
    "correct_pred = tf.equal(tf.argmax(pred,axis=1), tf.argmax(y,axis=1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# saver\n",
    "saver = tf.train.Saver(max_to_keep=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1, Minibatch Loss= 176.988373, Training Accuracy= 0.67969\n",
      "Step 1000, Minibatch Loss= 0.457291, Training Accuracy= 0.82031\n",
      "Step 2000, Minibatch Loss= 0.637155, Training Accuracy= 0.64844\n",
      "Step 3000, Minibatch Loss= 0.014912, Training Accuracy= 1.00000\n",
      "Step 4000, Minibatch Loss= 0.482683, Training Accuracy= 0.80469\n",
      "Step 5000, Minibatch Loss= 0.356878, Training Accuracy= 0.85156\n",
      "Step 6000, Minibatch Loss= 0.015085, Training Accuracy= 1.00000\n",
      "Step 7000, Minibatch Loss= 0.259261, Training Accuracy= 0.90625\n",
      "Step 8000, Minibatch Loss= 0.266745, Training Accuracy= 0.88281\n",
      "Step 9000, Minibatch Loss= 0.017243, Training Accuracy= 1.00000\n",
      "Step 10000, Minibatch Loss= 0.145362, Training Accuracy= 0.95312\n",
      "Step 11000, Minibatch Loss= 0.225585, Training Accuracy= 0.92188\n",
      "Step 12000, Minibatch Loss= 0.019390, Training Accuracy= 0.99219\n",
      "Step 13000, Minibatch Loss= 0.115287, Training Accuracy= 0.96094\n",
      "Step 14000, Minibatch Loss= 0.083211, Training Accuracy= 0.96875\n",
      "Step 15000, Minibatch Loss= 0.063192, Training Accuracy= 0.98438\n",
      "Step 16000, Minibatch Loss= 0.795692, Training Accuracy= 0.82031\n",
      "Step 17000, Minibatch Loss= 0.036655, Training Accuracy= 0.98438\n",
      "Step 18000, Minibatch Loss= 0.029554, Training Accuracy= 1.00000\n",
      "Step 19000, Minibatch Loss= 0.000014, Training Accuracy= 1.00000\n",
      "Step 20000, Minibatch Loss= 0.010081, Training Accuracy= 1.00000\n",
      "Step 21000, Minibatch Loss= 0.006265, Training Accuracy= 1.00000\n",
      "Step 22000, Minibatch Loss= 0.001883, Training Accuracy= 1.00000\n",
      "Step 23000, Minibatch Loss= 0.005612, Training Accuracy= 1.00000\n",
      "Step 24000, Minibatch Loss= 0.006751, Training Accuracy= 1.00000\n",
      "Step 25000, Minibatch Loss= 0.000387, Training Accuracy= 1.00000\n",
      "Step 26000, Minibatch Loss= 0.002655, Training Accuracy= 1.00000\n",
      "Step 27000, Minibatch Loss= 0.002415, Training Accuracy= 1.00000\n",
      "Step 28000, Minibatch Loss= 0.000513, Training Accuracy= 1.00000\n",
      "Step 29000, Minibatch Loss= 0.001438, Training Accuracy= 1.00000\n",
      "Step 30000, Minibatch Loss= 0.001493, Training Accuracy= 1.00000\n",
      "Step 31000, Minibatch Loss= 0.001094, Training Accuracy= 1.00000\n",
      "Step 32000, Minibatch Loss= 0.000543, Training Accuracy= 1.00000\n",
      "Step 33000, Minibatch Loss= 0.000803, Training Accuracy= 1.00000\n",
      "Step 34000, Minibatch Loss= 0.000943, Training Accuracy= 1.00000\n",
      "Step 35000, Minibatch Loss= 0.000694, Training Accuracy= 1.00000\n",
      "Step 36000, Minibatch Loss= 0.001052, Training Accuracy= 1.00000\n",
      "Step 37000, Minibatch Loss= 0.000684, Training Accuracy= 1.00000\n",
      "Step 38000, Minibatch Loss= 0.000122, Training Accuracy= 1.00000\n",
      "Step 39000, Minibatch Loss= 0.000212, Training Accuracy= 1.00000\n",
      "Step 40000, Minibatch Loss= 0.000715, Training Accuracy= 1.00000\n",
      "Step 41000, Minibatch Loss= 0.000611, Training Accuracy= 1.00000\n",
      "Step 42000, Minibatch Loss= 0.000200, Training Accuracy= 1.00000\n",
      "Step 43000, Minibatch Loss= 0.001146, Training Accuracy= 1.00000\n",
      "Step 44000, Minibatch Loss= 0.000349, Training Accuracy= 1.00000\n",
      "Step 45000, Minibatch Loss= 0.000055, Training Accuracy= 1.00000\n",
      "Step 46000, Minibatch Loss= 0.000258, Training Accuracy= 1.00000\n",
      "Step 47000, Minibatch Loss= 0.000127, Training Accuracy= 1.00000\n",
      "Step 48000, Minibatch Loss= 0.000041, Training Accuracy= 1.00000\n",
      "Step 49000, Minibatch Loss= 0.000163, Training Accuracy= 1.00000\n",
      "Step 50000, Minibatch Loss= 0.000038, Training Accuracy= 1.00000\n",
      "Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "    \n",
    "    #saver.restore(sess, \"./Model/model.ckpt\")\n",
    "\n",
    "    for step in range(1, training_steps+1):\n",
    "        batch_x, batch_y, batch_seqlen = trainset.next(batch_size)\n",
    "        # Run optimization op (backprop)\n",
    "        sess.run(optimizer, feed_dict={x: batch_x, \n",
    "                                       y: batch_y, \n",
    "                                       seqlen: batch_seqlen})\n",
    "        if step % display_step == 0 or step == 1:\n",
    "            # Calculate batch accuracy & loss\n",
    "            acc, loss = sess.run([accuracy, cost], feed_dict={x: batch_x, \n",
    "                                                              y: batch_y, \n",
    "                                                              seqlen: batch_seqlen})\n",
    "            \n",
    "            print(\"Step \" + str(step) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.5f}\".format(acc))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "    \n",
    "    # Save Model\n",
    "    saver.save(sess, \"Model/model.ckpt\")\n",
    "    \n",
    "    # Get Test Data\n",
    "    test_data = testset.data\n",
    "    test_label = testset.labels\n",
    "    test_seqlen = testset.seqlen\n",
    "\n",
    "    # Calculate accuracy\n",
    "    #print(\"Testing Accuracy:\", sess.run(accuracy, feed_dict={x: test_data, \n",
    "    #                                                         y: test_label, \n",
    "    #                                                         seqlen: test_seqlen}))\n",
    "    \n",
    "    # Calculate loss\n",
    "    #print(\"Testing Loss:\", sess.run(cost, feed_dict={x: test_data, \n",
    "    #                                                 y: test_label, \n",
    "    #                                                 seqlen: test_seqlen}))\n",
    "    \n",
    "    # Calculate output\n",
    "    test_result = sess.run(prob_pred, feed_dict={x: test_data, seqlen: test_seqlen})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from Model/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    saver.restore(sess, \"Model/model.ckpt\")\n",
    " \n",
    "    # Get Test Data\n",
    "    test_data = testset.data\n",
    "    test_label = testset.labels\n",
    "    test_seqlen = testset.seqlen\n",
    "\n",
    "    # Calculate output\n",
    "    test_result = sess.run(prob_pred, feed_dict={x: test_data, seqlen: test_seqlen})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test_result.csv', 'w', newline='') as csvFile:\n",
    "    writer = csv.writer(csvFile)\n",
    "    fileheader = [\"id\", \"realDonaldTrump\", \"HillaryClinton\"]\n",
    "    writer.writerow(fileheader)\n",
    "    for i in range(len(test_result)):\n",
    "        #onerow = [i, tweetList_test[i],test_result[i][1], test_result[i][0]]\n",
    "        writer.writerow([i, test_result[i][1], test_result[i][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
