{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 4743\n",
      "N = 4743\n",
      "N_test = 1701\n",
      "M = 8787\n",
      "N = 4743\n",
      "N_test = 1701\n",
      "\n",
      "when donald trump goes low   register to vote \n",
      "[2538, 6577, 4050, 5004, 6457, 2419, 7782, 4991]\n",
      "\n",
      "its nationalvoterregistrationday  celebrate by registering to vote â†’ \n",
      "[328, 7221, 192, 5882, 2927, 7782, 4991, 2759]\n",
      "\n",
      "N_train = 3794\n",
      "N_valid = 948\n"
     ]
    }
   ],
   "source": [
    "## read text data from file\n",
    "\n",
    "import csv\n",
    "import string\n",
    "\n",
    "labelList = []\n",
    "tweetList = []\n",
    "tweetList_test = []\n",
    "\n",
    "with open('train.csv', encoding='utf-8') as trainfile:\n",
    "    reader = csv.reader(trainfile)\n",
    "    for row in reader:\n",
    "        if row[0] == 'HillaryClinton':\n",
    "            labelList.append(1)\n",
    "        elif row[0] == 'realDonaldTrump':\n",
    "            labelList.append(0)\n",
    "            \n",
    "        if row[1] != 'tweet':\n",
    "            tweetList.append(row[1])\n",
    "            \n",
    "with open('test.csv', encoding='utf-8') as testfile:\n",
    "    reader = csv.reader(testfile)\n",
    "    for row in reader: \n",
    "        if row[1] != 'tweet':\n",
    "            tweetList_test.append(row[1])\n",
    "\n",
    "N = len(tweetList)\n",
    "print('N = %d' % len(tweetList))\n",
    "print('N = %d' % len(labelList))\n",
    "\n",
    "N_test = len(tweetList_test)\n",
    "print('N_test = %d' % len(tweetList_test))\n",
    "\n",
    "##############################################################\n",
    "\n",
    "## corpus\n",
    "import string\n",
    "\n",
    "def refineWordsList(tweetList):\n",
    "    ret = []\n",
    "    for tweet in tweetList:\n",
    "        step1 = tweet.split('http',1)[0]\n",
    "        step2 = step1.lower()\n",
    "        step3 = step2.translate(str.maketrans('.',' '))\n",
    "        step4 = step3.translate(str.maketrans('','',string.punctuation))\n",
    "        ret.append(step4)\n",
    "    return ret\n",
    "\n",
    "tweetList = refineWordsList(tweetList)\n",
    "tweetList_test = refineWordsList(tweetList_test)\n",
    "\n",
    "corpus = set()\n",
    "\n",
    "for tweet in tweetList:\n",
    "    words = tweet.split();\n",
    "    corpus.update(words)\n",
    "\n",
    "corpus = list(corpus)\n",
    "M = len(corpus)\n",
    "print('M = %d' % M)\n",
    "\n",
    "##############################################################\n",
    "\n",
    "## Label Encoding\n",
    "\n",
    "hashEncoder = {}\n",
    "i = 0\n",
    "for word in corpus:\n",
    "    hashEncoder[word] = i\n",
    "    i += 1\n",
    "\n",
    "# train\n",
    "tweetList_encoded = []\n",
    "for tweet in tweetList:\n",
    "    tweet_encoded = []\n",
    "    for word in tweet.split():\n",
    "        word_encoded = hashEncoder[word]\n",
    "        tweet_encoded.append(word_encoded)\n",
    "    tweetList_encoded.append(tweet_encoded)\n",
    "    \n",
    "# test\n",
    "tweetList_test_encoded = []\n",
    "for tweet in tweetList_test:\n",
    "    tweet_encoded = []\n",
    "    for word in tweet.split():\n",
    "        if hashEncoder.get(word) != None :\n",
    "            word_encoded = hashEncoder[word]\n",
    "            tweet_encoded.append(word_encoded)\n",
    "    tweetList_test_encoded.append(tweet_encoded)\n",
    "\n",
    "def decodeTweet(tweet_encoded):\n",
    "    decodeList = []\n",
    "    for word_encoded in tweetList_encoded[5]:\n",
    "        decodeList.append(list(hashEncoder.keys())[list(hashEncoder.values()).index(word_encoded)])\n",
    "    return decodeList\n",
    "    \n",
    "print('N = %d' % len(tweetList_encoded))\n",
    "print('N_test = %d' % len(tweetList_test_encoded))\n",
    "print()\n",
    "print(tweetList[5])\n",
    "print(tweetList_encoded[5])\n",
    "#print(decodeTweet(tweetList_encoded[5]))\n",
    "print()\n",
    "print(tweetList_test[5])\n",
    "print(tweetList_test_encoded[5])\n",
    "print()\n",
    "\n",
    "##############################################################\n",
    "\n",
    "N_train = (int)(0.8 * N)\n",
    "N_valid = N - (int)(0.8 * N) - 1\n",
    "\n",
    "tweetList_encoded_train = [tweetList_encoded[x] for x in range (N_train)]\n",
    "tweetList_encoded_valid = [tweetList_encoded[x] for x in range (N_train + 1, N)]\n",
    "\n",
    "labelList_train = [labelList[x] for x in range (N_train)]\n",
    "labelList_valid = [labelList[x] for x in range (N_train + 1, N)]\n",
    "\n",
    "print('N_train = %d' % N_train)\n",
    "print('N_valid = %d' % N_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceData(object):\n",
    "    \n",
    "    def __init__(self, n_samples=1000, max_seq_len = 31, x_list=[], t_list=[]):\n",
    "        \n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        self.seqlen = []\n",
    "        \n",
    "        for i in range(n_samples):\n",
    "            len1 = len(x_list[i])\n",
    "            self.seqlen.append(len1)\n",
    "            s = [[x_list[i][j]] for j in range(len1)]\n",
    "            s += [[0] for j in range(max_seq_len - len1 )]\n",
    "            self.data.append(s)\n",
    "            self.labels.append([t_list[i], 1 - t_list[i]])\n",
    "            \n",
    "        self.batch_id = 0\n",
    "\n",
    "    def next(self, batch_size):\n",
    "\n",
    "        if self.batch_id == len(self.data):\n",
    "            self.batch_id = 0\n",
    "        batch_data = (self.data[self.batch_id:min(self.batch_id +\n",
    "                                                  batch_size, len(self.data))])\n",
    "        batch_labels = (self.labels[self.batch_id:min(self.batch_id +\n",
    "                                                  batch_size, len(self.data))])\n",
    "        batch_seqlen = (self.seqlen[self.batch_id:min(self.batch_id +\n",
    "                                                  batch_size, len(self.data))])\n",
    "        self.batch_id = min(self.batch_id + batch_size, len(self.data))\n",
    "        return batch_data, batch_labels, batch_seqlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========\n",
    "#   MODEL\n",
    "# ==========\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "training_steps = 10000\n",
    "batch_size = 128\n",
    "display_step = 100\n",
    "\n",
    "# Network Parameters\n",
    "seq_max_len = 31 # Sequence max length\n",
    "n_hidden = 31\n",
    "n_hidden_1 = 3 # 1st layer number of neurons\n",
    "n_hidden_2 = 3 # 2nd layer number of neurons\n",
    "n_classes = 2 # linear sequence or notstate_size = 1#20\n",
    "\n",
    "state_size = 1#20\n",
    "\n",
    "trainset = SequenceData(n_samples = N_train,\n",
    "                           max_seq_len = seq_max_len,\n",
    "                           x_list = tweetList_encoded_train,\n",
    "                           t_list = labelList_train)\n",
    "\n",
    "testset = SequenceData(n_samples = N_valid,\n",
    "                          max_seq_len = seq_max_len,\n",
    "                          x_list = tweetList_encoded_valid,\n",
    "                          t_list = labelList_valid)\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(\"float\", [None, seq_max_len, state_size])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "# A placeholder for indicating each sequence length\n",
    "seqlen = tf.placeholder(tf.int32, [None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_hidden, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_net(x, weights, biases):\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamicRNN(x, seqlen, weights, biases):\n",
    "\n",
    "    # Prepare data shape to match `rnn` function requirements\n",
    "    # Current data input shape: (batch_size, n_steps, n_input)\n",
    "    # Required shape: 'n_steps' tensors list of shape (batch_size, n_input)\n",
    "    \n",
    "    # Unstack to get a list of 'n_steps' tensors of shape (batch_size, n_input)\n",
    "    x = tf.unstack(x, seq_max_len, 1)\n",
    "\n",
    "    # Define a lstm cell with tensorflow\n",
    "    lstm_cell = tf.contrib.rnn.BasicLSTMCell(n_hidden)\n",
    "\n",
    "    # Get lstm cell output, providing 'sequence_length' will perform dynamic\n",
    "    # calculation.\n",
    "    outputs, states = tf.contrib.rnn.static_rnn(lstm_cell, x, dtype=tf.float32,\n",
    "                                sequence_length=seqlen)\n",
    "\n",
    "    # When performing dynamic calculation, we must retrieve the last\n",
    "    # dynamically computed output, i.e., if a sequence length is 10, we need\n",
    "    # to retrieve the 10th output.\n",
    "    # However TensorFlow doesn't support advanced indexing yet, so we build\n",
    "    # a custom op that for each sample in batch size, get its length and\n",
    "    # get the corresponding relevant output.\n",
    "\n",
    "    # 'outputs' is a list of output at every timestep, we pack them in a Tensor\n",
    "    # and change back dimension to [batch_size, n_step, n_input]\n",
    "    outputs = tf.stack(outputs)\n",
    "    outputs = tf.transpose(outputs, [1, 0, 2])\n",
    "\n",
    "    # Hack to build the indexing and retrieve the right output.\n",
    "    batch_size = tf.shape(outputs)[0]\n",
    "    # Start indices for each sample\n",
    "    index = tf.range(0, batch_size) * seq_max_len + (seqlen - 1)\n",
    "    # Indexing\n",
    "    outputs = tf.gather(tf.reshape(outputs, [-1, n_hidden]), index)\n",
    "\n",
    "    return neural_net(outputs, weights, biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:96: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "pred = dynamicRNN(x, seqlen, weights, biases)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1, Minibatch Loss= 7.989432, Training Accuracy= 0.32031\n",
      "Step 100, Minibatch Loss= 0.674643, Training Accuracy= 0.60156\n",
      "Step 200, Minibatch Loss= 0.624832, Training Accuracy= 0.68750\n",
      "Step 300, Minibatch Loss= 0.230898, Training Accuracy= 1.00000\n",
      "Step 400, Minibatch Loss= 0.675002, Training Accuracy= 0.60156\n",
      "Step 500, Minibatch Loss= 0.625133, Training Accuracy= 0.68750\n",
      "Step 600, Minibatch Loss= 0.261633, Training Accuracy= 1.00000\n",
      "Step 700, Minibatch Loss= 0.675168, Training Accuracy= 0.60156\n",
      "Step 800, Minibatch Loss= 0.624908, Training Accuracy= 0.68750\n",
      "Step 900, Minibatch Loss= 0.292252, Training Accuracy= 1.00000\n",
      "Step 1000, Minibatch Loss= 0.675133, Training Accuracy= 0.60156\n",
      "Step 1100, Minibatch Loss= 0.624631, Training Accuracy= 0.68750\n",
      "Step 1200, Minibatch Loss= 0.322621, Training Accuracy= 1.00000\n",
      "Step 1300, Minibatch Loss= 0.674891, Training Accuracy= 0.60156\n",
      "Step 1400, Minibatch Loss= 0.624367, Training Accuracy= 0.68750\n",
      "Step 1500, Minibatch Loss= 0.352564, Training Accuracy= 1.00000\n",
      "Step 1600, Minibatch Loss= 0.674462, Training Accuracy= 0.60156\n",
      "Step 1700, Minibatch Loss= 0.624154, Training Accuracy= 0.68750\n",
      "Step 1800, Minibatch Loss= 0.381973, Training Accuracy= 1.00000\n",
      "Step 1900, Minibatch Loss= 0.673886, Training Accuracy= 0.60156\n",
      "Step 2000, Minibatch Loss= 0.624014, Training Accuracy= 0.68750\n",
      "Step 2100, Minibatch Loss= 0.410755, Training Accuracy= 1.00000\n",
      "Step 2200, Minibatch Loss= 0.673216, Training Accuracy= 0.60156\n",
      "Step 2300, Minibatch Loss= 0.623979, Training Accuracy= 0.68750\n",
      "Step 2400, Minibatch Loss= 0.438800, Training Accuracy= 1.00000\n",
      "Step 2500, Minibatch Loss= 0.672518, Training Accuracy= 0.60156\n",
      "Step 2600, Minibatch Loss= 0.624046, Training Accuracy= 0.68750\n",
      "Step 2700, Minibatch Loss= 0.465959, Training Accuracy= 1.00000\n",
      "Step 2800, Minibatch Loss= 0.671885, Training Accuracy= 0.60156\n",
      "Step 2900, Minibatch Loss= 0.624076, Training Accuracy= 0.68750\n",
      "Step 3000, Minibatch Loss= 0.492192, Training Accuracy= 1.00000\n",
      "Step 3100, Minibatch Loss= 0.671242, Training Accuracy= 0.60156\n",
      "Step 3200, Minibatch Loss= 0.624080, Training Accuracy= 0.68750\n",
      "Step 3300, Minibatch Loss= 0.517417, Training Accuracy= 1.00000\n",
      "Step 3400, Minibatch Loss= 0.670651, Training Accuracy= 0.60156\n",
      "Step 3500, Minibatch Loss= 0.624270, Training Accuracy= 0.68750\n",
      "Step 3600, Minibatch Loss= 0.541664, Training Accuracy= 1.00000\n",
      "Step 3700, Minibatch Loss= 0.670074, Training Accuracy= 0.60156\n",
      "Step 3800, Minibatch Loss= 0.624272, Training Accuracy= 0.68750\n",
      "Step 3900, Minibatch Loss= 0.564196, Training Accuracy= 1.00000\n",
      "Step 4000, Minibatch Loss= 0.669636, Training Accuracy= 0.60156\n",
      "Step 4100, Minibatch Loss= 0.624290, Training Accuracy= 0.68750\n",
      "Step 4200, Minibatch Loss= 0.585289, Training Accuracy= 1.00000\n",
      "Step 4300, Minibatch Loss= 0.669274, Training Accuracy= 0.60156\n",
      "Step 4400, Minibatch Loss= 0.624296, Training Accuracy= 0.68750\n",
      "Step 4500, Minibatch Loss= 0.604912, Training Accuracy= 1.00000\n",
      "Step 4600, Minibatch Loss= 0.668982, Training Accuracy= 0.61719\n",
      "Step 4700, Minibatch Loss= 0.624223, Training Accuracy= 0.68750\n",
      "Step 4800, Minibatch Loss= 0.623170, Training Accuracy= 1.00000\n",
      "Step 4900, Minibatch Loss= 0.668568, Training Accuracy= 0.61719\n",
      "Step 5000, Minibatch Loss= 0.623155, Training Accuracy= 0.68750\n",
      "Step 5100, Minibatch Loss= 0.638354, Training Accuracy= 1.00000\n",
      "Step 5200, Minibatch Loss= 0.668054, Training Accuracy= 0.61719\n",
      "Step 5300, Minibatch Loss= 0.623111, Training Accuracy= 0.68750\n",
      "Step 5400, Minibatch Loss= 0.652904, Training Accuracy= 0.98780\n",
      "Step 5500, Minibatch Loss= 0.667888, Training Accuracy= 0.61719\n",
      "Step 5600, Minibatch Loss= 0.623173, Training Accuracy= 0.68750\n",
      "Step 5700, Minibatch Loss= 0.666153, Training Accuracy= 0.96341\n",
      "Step 5800, Minibatch Loss= 0.667746, Training Accuracy= 0.61719\n",
      "Step 5900, Minibatch Loss= 0.623237, Training Accuracy= 0.68750\n",
      "Step 6000, Minibatch Loss= 0.678168, Training Accuracy= 0.92683\n",
      "Step 6100, Minibatch Loss= 0.667623, Training Accuracy= 0.61719\n",
      "Step 6200, Minibatch Loss= 0.623302, Training Accuracy= 0.68750\n",
      "Step 6300, Minibatch Loss= 0.689033, Training Accuracy= 0.87805\n",
      "Step 6400, Minibatch Loss= 0.667515, Training Accuracy= 0.61719\n",
      "Step 6500, Minibatch Loss= 0.623368, Training Accuracy= 0.68750\n",
      "Step 6600, Minibatch Loss= 0.698839, Training Accuracy= 0.01220\n",
      "Step 6700, Minibatch Loss= 0.667418, Training Accuracy= 0.61719\n",
      "Step 6800, Minibatch Loss= 0.623431, Training Accuracy= 0.68750\n",
      "Step 6900, Minibatch Loss= 0.707676, Training Accuracy= 0.00000\n",
      "Step 7000, Minibatch Loss= 0.667329, Training Accuracy= 0.61719\n",
      "Step 7100, Minibatch Loss= 0.623493, Training Accuracy= 0.68750\n",
      "Step 7200, Minibatch Loss= 0.715633, Training Accuracy= 0.00000\n",
      "Step 7300, Minibatch Loss= 0.667249, Training Accuracy= 0.61719\n",
      "Step 7400, Minibatch Loss= 0.623551, Training Accuracy= 0.68750\n",
      "Step 7500, Minibatch Loss= 0.722793, Training Accuracy= 0.00000\n",
      "Step 7600, Minibatch Loss= 0.667175, Training Accuracy= 0.61719\n",
      "Step 7700, Minibatch Loss= 0.623606, Training Accuracy= 0.68750\n",
      "Step 7800, Minibatch Loss= 0.729238, Training Accuracy= 0.00000\n",
      "Step 7900, Minibatch Loss= 0.667106, Training Accuracy= 0.61719\n",
      "Step 8000, Minibatch Loss= 0.623658, Training Accuracy= 0.68750\n",
      "Step 8100, Minibatch Loss= 0.735040, Training Accuracy= 0.00000\n",
      "Step 8200, Minibatch Loss= 0.667042, Training Accuracy= 0.61719\n",
      "Step 8300, Minibatch Loss= 0.623707, Training Accuracy= 0.68750\n",
      "Step 8400, Minibatch Loss= 0.740269, Training Accuracy= 0.00000\n",
      "Step 8500, Minibatch Loss= 0.666982, Training Accuracy= 0.61719\n",
      "Step 8600, Minibatch Loss= 0.623753, Training Accuracy= 0.68750\n",
      "Step 8700, Minibatch Loss= 0.744988, Training Accuracy= 0.00000\n",
      "Step 8800, Minibatch Loss= 0.666926, Training Accuracy= 0.61719\n",
      "Step 8900, Minibatch Loss= 0.623795, Training Accuracy= 0.68750\n",
      "Step 9000, Minibatch Loss= 0.749255, Training Accuracy= 0.00000\n",
      "Step 9100, Minibatch Loss= 0.666873, Training Accuracy= 0.61719\n",
      "Step 9200, Minibatch Loss= 0.623835, Training Accuracy= 0.68750\n",
      "Step 9300, Minibatch Loss= 0.753122, Training Accuracy= 0.00000\n",
      "Step 9400, Minibatch Loss= 0.666822, Training Accuracy= 0.61719\n",
      "Step 9500, Minibatch Loss= 0.623871, Training Accuracy= 0.68750\n"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "\n",
    "    for step in range(1, training_steps+1):\n",
    "        batch_x, batch_y, batch_seqlen = trainset.next(batch_size)\n",
    "        # Run optimization op (backprop)\n",
    "        sess.run(optimizer, feed_dict={x: batch_x, y: batch_y,\n",
    "                                       seqlen: batch_seqlen})\n",
    "        if step % display_step == 0 or step == 1:\n",
    "            # Calculate batch accuracy & loss\n",
    "            acc, loss = sess.run([accuracy, cost], feed_dict={x: batch_x, y: batch_y,\n",
    "                                                seqlen: batch_seqlen})\n",
    "            print(\"Step \" + str(step) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.5f}\".format(acc))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Calculate accuracy\n",
    "    test_data = testset.data\n",
    "    test_label = testset.labels\n",
    "    test_seqlen = testset.seqlen\n",
    "    print(\"Testing Accuracy:\", \\\n",
    "        sess.run(accuracy, feed_dict={x: test_data, y: test_label,\n",
    "                                      seqlen: test_seqlen}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
