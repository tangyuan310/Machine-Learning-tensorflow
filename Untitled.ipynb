{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:96: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.296875 6.25454\n",
      "1.0 0.0730614\n",
      "0.742188 1.82456\n",
      "0.9375 0.276418\n",
      "1.0 0.00639619\n",
      "0.992188 0.0254632\n",
      "1.0 0.00201033\n",
      "1.0 0.000632242\n",
      "1.0 0.000301399\n",
      "1.0 0.000138176\n",
      "1.0 0.00324259\n",
      "1.0 0.000312304\n",
      "1.0 0.0020797\n",
      "1.0 0.000997086\n",
      "1.0 5.70596e-05\n",
      "1.0 3.23813e-06\n",
      "1.0 0.00200177\n",
      "1.0 1.49784e-06\n",
      "1.0 3.64976e-05\n",
      "1.0 0.000184052\n",
      "1.0 2.46132e-05\n",
      "train over!\n",
      "INFO:tensorflow:Restoring parameters from ./Model/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "with open('train.csv', \"rt\", encoding='utf-8', errors='ignore') as csvfile:\n",
    "    twtreader = csv.reader(csvfile)\n",
    "\n",
    "    trainlistoriginal = list(twtreader)\n",
    "    trainlistoriginal = trainlistoriginal[1:]\n",
    "\n",
    "    traintar = [None] * len(trainlistoriginal)\n",
    "    trainlist = [None] * len(trainlistoriginal)\n",
    "\n",
    "    for i in range(0, len(trainlistoriginal)):\n",
    "        traintar[i] = trainlistoriginal[i][0]\n",
    "        trainlist[i] = trainlistoriginal[i][1]\n",
    "    #eliminate retweets\n",
    "    str = \"http\"\n",
    "    for i in range(0, len(trainlist)):\n",
    "        strcount = trainlist[i].count(str)\n",
    "        if strcount > 0:\n",
    "            for j in range(0, strcount):\n",
    "                mystr = trainlist[i]\n",
    "                ind = mystr.index(str, 0)\n",
    "                trainlist[i] = trainlist[i][0:ind-1] + trainlist[i][ind + 23-1: -1]\n",
    "    #classify training target\n",
    "    trainumtar = [[0 for i in range(2)] for i in range(len(traintar))]\n",
    "    for i in range(0, len(traintar)):\n",
    "        if traintar[i] == \"HillaryClinton\":\n",
    "            trainumtar[i][0] = 1\n",
    "        else:\n",
    "            trainumtar[i][1] = 1\n",
    "# words tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "for i in range(0, len(trainlist)):\n",
    "    trainlist[i] = tokenizer.tokenize(trainlist[i])\n",
    "\n",
    "# find sentenses' max length\n",
    "maxlen = 0\n",
    "for tweet in trainlist:\n",
    "    if len(tweet) > maxlen:\n",
    "        maxlen = len(tweet)\n",
    "\n",
    "trainlistallwords = set()\n",
    "for i in range(0, len(trainlist)):\n",
    "    trainlistallwords.update(trainlist[i])\n",
    "\n",
    "trainlistallwords = list(trainlistallwords)\n",
    "N = len(trainlistallwords)\n",
    "#make a dictionary\n",
    "dict = {}\n",
    "wordnum = 0\n",
    "for word in trainlistallwords:\n",
    "    dict[word] = wordnum\n",
    "    wordnum += 1\n",
    "\n",
    "# change the words batch to number batch\n",
    "def getbatchlabel(index):\n",
    "    batch_label = [0] * maxlen\n",
    "    for i in range(len(trainlist[index])):\n",
    "        batch_label[i] = dict[trainlist[index][i]]\n",
    "    return batch_label\n",
    "\n",
    "traindata = []\n",
    "for i in range(len(trainlist)):\n",
    "    traindata.append(getbatchlabel(i))\n",
    "\n",
    "# get test data\n",
    "with open('test.csv', \"rt\", encoding='utf-8', errors='ignore') as csvfile:\n",
    "    twtreader = csv.reader(csvfile)\n",
    "\n",
    "    testlistoriginal = list(twtreader)\n",
    "    testlistoriginal = testlistoriginal[1:]\n",
    "\n",
    "    testlist = [None] * len(testlistoriginal)\n",
    "\n",
    "    for i in range(0, len(testlistoriginal)):\n",
    "        testlist[i] = testlistoriginal[i][1]\n",
    "\n",
    "    str = \"http\"\n",
    "    for i in range(0, len(testlist)):\n",
    "        strcount = testlist[i].count(str)\n",
    "        if strcount > 0:\n",
    "            for j in range(0, strcount):\n",
    "                mystr = testlist[i]\n",
    "                ind = mystr.index(str, 0)\n",
    "                testlist[i] = testlist[i][0:ind - 1] + testlist[i][ind + 23 - 1: -1]\n",
    "\n",
    "    from nltk.tokenize import RegexpTokenizer\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    for i in range(0, len(testlist)):\n",
    "        testlist[i] = tokenizer.tokenize(testlist[i])\n",
    "\n",
    "def gettestdata(testlist):\n",
    "    testlist_processed = []\n",
    "    for i in range(len(testlist)):\n",
    "        new_test_list = []\n",
    "        for j in range(len(testlist[i])):\n",
    "            if dict.get(testlist[i][j]) != None:\n",
    "                new_test_list.append(testlist[i][j])\n",
    "        testlist_processed.append(new_test_list)\n",
    "\n",
    "    test_data = []\n",
    "    for i in range(len(testlist_processed)):\n",
    "        test_label = [0] * maxlen\n",
    "        for j in range(min(len(testlist_processed[i]), maxlen)):\n",
    "            test_label[j] = dict[testlist_processed[i][j]]\n",
    "        test_data.append(test_label)\n",
    "\n",
    "    test_len = []\n",
    "    for i in range(len(testlist)):\n",
    "        test_len.append(min(len(testlist_processed[i]), maxlen))\n",
    "    return test_data, test_len\n",
    "\n",
    "# get train data\n",
    "def gettrainbatch(batchi, batch_size):\n",
    "    batch_x = []\n",
    "    batch_y = []\n",
    "    batch_len = []\n",
    "    if (batchi + 1) * batch_size > len(traindata):\n",
    "        out_length = len(traindata) - batchi * batch_size\n",
    "        for i in range(out_length):\n",
    "            batch_x.append(traindata[batchi * batch_size + i])\n",
    "            batch_y.append(trainumtar[batchi * batch_size + i])\n",
    "        for i in range(out_length):\n",
    "            batch_len.append(len(trainlist[batchi * batch_size + i]))\n",
    "    else:\n",
    "        for i in range(batch_size):\n",
    "            batch_x.append(traindata[batchi * batch_size + i])\n",
    "            batch_y.append(trainumtar[batchi * batch_size + i])\n",
    "        for i in range(batch_size):\n",
    "            batch_len.append(len(trainlist[batchi * batch_size + i]))\n",
    "    return batch_x, batch_y, batch_len\n",
    "\n",
    "import tensorflow as tf\n",
    "vocalbulary_size = len(trainlistallwords)\n",
    "embedding_size = 25\n",
    "lstm_size = 25\n",
    "senten_words_num = maxlen\n",
    "class_num = 2\n",
    "\n",
    "X = tf.placeholder(tf.int32, [None, maxlen])\n",
    "Y = tf.placeholder(tf.float32, [None, 2])\n",
    "sentense_length = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "import numpy as np\n",
    "# weights = tf.Variable(tf.truncated_normal([embedding_size, class_num], stddev=1, dtype=tf.float32))\n",
    "# bias = tf.Variable(tf.constant(0.0, shape=[class_num], dtype=tf.float32))\n",
    "\n",
    "embeddings = tf.Variable(tf.random_uniform([vocalbulary_size, embedding_size], -1.0, 1.0))\n",
    "inputs = tf.nn.embedding_lookup(embeddings, X)\n",
    "def forwardNN(last_rnn_output):\n",
    "    lay_1_num = 50\n",
    "    lay_2_num = 100\n",
    "    weights_1 = tf.Variable(tf.truncated_normal([embedding_size, lay_1_num], stddev=np.sqrt(2.0/lay_1_num), dtype=tf.float32))\n",
    "    bias_1 = tf.Variable(tf.constant(0.0, shape=[lay_1_num], dtype=tf.float32))\n",
    "    result_1 = tf.matmul(last_rnn_output, weights_1) + bias_1\n",
    "    weights_2 = tf.Variable(tf.truncated_normal([lay_1_num, lay_2_num], stddev=np.sqrt(2.0/lay_2_num), dtype=tf.float32))\n",
    "    bias_2 = tf.Variable(tf.constant(0.0, shape=[lay_2_num], dtype=tf.float32))\n",
    "    result_2 = tf.matmul(result_1, weights_2) + bias_2\n",
    "    out_weights = tf.Variable(tf.truncated_normal([lay_2_num, class_num], stddev=np.sqrt(2.0/class_num), dtype=tf.float32))\n",
    "    out_bias = tf.Variable(tf.constant(0.0, shape=[class_num], dtype=tf.float32))\n",
    "    out_result = tf.matmul(result_2, out_weights) + out_bias\n",
    "    l2_loss = tf.nn.l2_loss(weights_1) + tf.nn.l2_loss(out_weights) + tf.nn.l2_loss(weights_2)\n",
    "    return out_result, l2_loss\n",
    "\n",
    "lstm_cells = tf.contrib.rnn.BasicLSTMCell(num_units=lstm_size)\n",
    "outputs, state = tf.nn.dynamic_rnn(cell=lstm_cells, inputs=inputs, dtype=tf.float32, sequence_length=sentense_length)\n",
    "outputs = tf.nn.dropout(outputs, keep_prob=0.6)\n",
    "last_output_idx = tf.range(tf.shape(outputs)[0]) * tf.shape(outputs)[1] + sentense_length - 1\n",
    "last_rnn_output = tf.gather(tf.reshape(outputs, [-1, lstm_size]), last_output_idx)\n",
    "\n",
    "# one_batch_predict = tf.nn.softmax(tf.matmul(last_rnn_output, weights) + bias)\n",
    "one_batch_predict = forwardNN(last_rnn_output)[0]\n",
    "classify = tf.nn.sigmoid()\n",
    "probability = tf.nn.softmax(one_batch_predict)\n",
    "l2_loss = forwardNN(last_rnn_output)[1]\n",
    "cross_entropy_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=one_batch_predict, labels=Y)) + 0.001 * l2_loss\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "starter_learning_rate = 0.1\n",
    "learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step, 2000, 0.96, staircase=True)\n",
    "learning_step = (tf.train.GradientDescentOptimizer(learning_rate).minimize(cross_entropy_loss))\n",
    "\n",
    "\n",
    "accurate_position = tf.equal(tf.argmax(Y, 1), tf.argmax(one_batch_predict, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(accurate_position, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=1)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    import math\n",
    "    import random\n",
    "    batch_size = 128\n",
    "    train_batch_num = math.ceil(len(trainlist) / batch_size)\n",
    "    for i in range(1, 2001):\n",
    "        for batchi in range(train_batch_num):\n",
    "            batch_x, batch_y, batch_len = gettrainbatch(batchi, batch_size)\n",
    "            sess.run(learning_step, feed_dict={X: batch_x, Y: batch_y, sentense_length: batch_len})\n",
    "        if i % 100 == 0 or i == 1:\n",
    "            batch_x, batch_y, batch_len = gettrainbatch(random.randint(0, train_batch_num-1), batch_size)\n",
    "            acc, loss = sess.run([accuracy, cross_entropy_loss], feed_dict={X: batch_x, Y: batch_y,\n",
    "                                                                  sentense_length: batch_len})\n",
    "            print(acc, loss)\n",
    "    print(\"train over!\")\n",
    "    saver.save(sess, \"Model/model.ckpt\")\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    saver.restore(sess, \"./Model/model.ckpt\")\n",
    "#Validation part\n",
    "    # val_x = []\n",
    "    # val_y = []\n",
    "    # val_len = []\n",
    "    # for i in range((train_batch_num - 5) * batch_size, len(trainlist)):\n",
    "    #     val_x.append(trainlist[i])\n",
    "    #     val_y.append(trainumtar[i])\n",
    "    # val_x, val_len = gettestdata(val_x)\n",
    "    # val_acc, val_loss = sess.run([accuracy, cross_entropy_loss], feed_dict={X: val_x, Y: val_y, sentense_length: val_len})\n",
    "    # print('val_acc:%f, val_loss:%f' % (val_acc, val_loss))\n",
    "\n",
    "#test part\n",
    "    testdata, test_len = gettestdata(testlist)\n",
    "    test_result = sess.run(probability, feed_dict={X: testdata, sentense_length: test_len})\n",
    "\n",
    "    csvFile = open(\"test_result.csv\", \"w\", newline='')\n",
    "    writer = csv.writer(csvFile)\n",
    "\n",
    "    fileheader = [\"id\", \"realDonaldTrump\", \"HillaryClinton\"]\n",
    "\n",
    "    writer.writerow(fileheader)\n",
    "\n",
    "    for i in range(len(test_result)):\n",
    "        writer.writerow([i, test_result[i][1], test_result[i][0]])\n",
    "\n",
    "    csvFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
