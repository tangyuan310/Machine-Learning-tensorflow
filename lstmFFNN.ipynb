{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 4743\n",
      "N = 4743\n",
      "N_test = 1701\n",
      "M = 8787\n",
      "N = 4743\n",
      "N_test = 1701\n",
      "\n",
      "when donald trump goes low   register to vote \n",
      "[2401, 7738, 5358, 8138, 122, 2994, 3026, 1289]\n",
      "\n",
      "its nationalvoterregistrationday  celebrate by registering to vote â†’ \n",
      "[694, 6083, 2711, 4934, 4669, 3026, 1289, 8604]\n",
      "\n",
      "N_train = 4268\n",
      "N_valid = 474\n"
     ]
    }
   ],
   "source": [
    "## read text data from file\n",
    "\n",
    "import csv\n",
    "import string\n",
    "\n",
    "labelList = []\n",
    "tweetList = []\n",
    "tweetList_test = []\n",
    "\n",
    "with open('train.csv', encoding='utf-8') as trainfile:\n",
    "    reader = csv.reader(trainfile)\n",
    "    for row in reader:\n",
    "        if row[0] == 'HillaryClinton':\n",
    "            labelList.append(1)\n",
    "        elif row[0] == 'realDonaldTrump':\n",
    "            labelList.append(0)\n",
    "            \n",
    "        if row[1] != 'tweet':\n",
    "            tweetList.append(row[1])\n",
    "            \n",
    "with open('test.csv', encoding='utf-8') as testfile:\n",
    "    reader = csv.reader(testfile)\n",
    "    for row in reader: \n",
    "        if row[1] != 'tweet':\n",
    "            tweetList_test.append(row[1])\n",
    "\n",
    "N = len(tweetList)\n",
    "print('N = %d' % len(tweetList))\n",
    "print('N = %d' % len(labelList))\n",
    "\n",
    "N_test = len(tweetList_test)\n",
    "print('N_test = %d' % len(tweetList_test))\n",
    "\n",
    "##############################################################\n",
    "\n",
    "## corpus\n",
    "import string\n",
    "\n",
    "def refineWordsList(tweetList):\n",
    "    ret = []\n",
    "    for tweet in tweetList:\n",
    "        step1 = tweet.split('http',1)[0]\n",
    "        step2 = step1.lower()\n",
    "        step3 = step2.translate(str.maketrans('.',' '))\n",
    "        step4 = step3.translate(str.maketrans('','',string.punctuation))\n",
    "        ret.append(step4)\n",
    "    return ret\n",
    "\n",
    "tweetList = refineWordsList(tweetList)\n",
    "tweetList_test = refineWordsList(tweetList_test)\n",
    "\n",
    "corpus = set()\n",
    "\n",
    "for tweet in tweetList:\n",
    "    words = tweet.split();\n",
    "    corpus.update(words)\n",
    "\n",
    "corpus = list(corpus)\n",
    "M = len(corpus)\n",
    "print('M = %d' % M)\n",
    "\n",
    "##############################################################\n",
    "\n",
    "## Label Encoding\n",
    "\n",
    "hashEncoder = {}\n",
    "i = 0\n",
    "for word in corpus:\n",
    "    hashEncoder[word] = i\n",
    "    i += 1\n",
    "\n",
    "# train\n",
    "tweetList_encoded = []\n",
    "for tweet in tweetList:\n",
    "    tweet_encoded = []\n",
    "    for word in tweet.split():\n",
    "        word_encoded = hashEncoder[word]\n",
    "        tweet_encoded.append(word_encoded)\n",
    "    tweetList_encoded.append(tweet_encoded)\n",
    "    \n",
    "# test\n",
    "tweetList_test_encoded = []\n",
    "for tweet in tweetList_test:\n",
    "    tweet_encoded = []\n",
    "    for word in tweet.split():\n",
    "        if hashEncoder.get(word) != None :\n",
    "            word_encoded = hashEncoder[word]\n",
    "            tweet_encoded.append(word_encoded)\n",
    "    tweetList_test_encoded.append(tweet_encoded)\n",
    "\n",
    "def decodeTweet(tweet_encoded):\n",
    "    decodeList = []\n",
    "    for word_encoded in tweetList_encoded[5]:\n",
    "        decodeList.append(list(hashEncoder.keys())[list(hashEncoder.values()).index(word_encoded)])\n",
    "    return decodeList\n",
    "    \n",
    "print('N = %d' % len(tweetList_encoded))\n",
    "print('N_test = %d' % len(tweetList_test_encoded))\n",
    "print()\n",
    "print(tweetList[5])\n",
    "print(tweetList_encoded[5])\n",
    "#print(decodeTweet(tweetList_encoded[5]))\n",
    "print()\n",
    "print(tweetList_test[5])\n",
    "print(tweetList_test_encoded[5])\n",
    "print()\n",
    "\n",
    "##############################################################\n",
    "\n",
    "# find sentenses' max length\n",
    "maxlen = 0\n",
    "for tweet in tweetList_encoded:\n",
    "    if len(tweet) > maxlen:\n",
    "        maxlen = len(tweet)\n",
    "\n",
    "##############################################################\n",
    "\n",
    "N_train = (int)(0.9 * N)\n",
    "N_valid = N - (int)(0.9 * N) - 1\n",
    "\n",
    "tweetList_encoded_train = [tweetList_encoded[x] for x in range (N_train)]\n",
    "tweetList_encoded_valid = [tweetList_encoded[x] for x in range (N_train + 1, N)]\n",
    "\n",
    "labelList_train = [labelList[x] for x in range (N_train)]\n",
    "labelList_valid = [labelList[x] for x in range (N_train + 1, N)]\n",
    "\n",
    "print('N_train = %d' % N_train)\n",
    "print('N_valid = %d' % N_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceData(object):\n",
    "    \n",
    "    def __init__(self, n_samples=1000, max_seq_len = 31, x_list=[], t_list=[]):\n",
    "        \n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        self.seqlen = []\n",
    "        \n",
    "        for i in range(n_samples):\n",
    "            len1 = len(x_list[i])\n",
    "            self.seqlen.append(len1)\n",
    "            s = [x_list[i][j] for j in range(len1)]\n",
    "            s += [0 for j in range(max_seq_len - len1)]\n",
    "            self.data.append(s)\n",
    "            \n",
    "            if len(t_list) != 0:\n",
    "                self.labels.append([t_list[i], 1 - t_list[i]])\n",
    "            else:\n",
    "                self.labels.append([1, 0])\n",
    "            \n",
    "        self.batch_id = 0\n",
    "\n",
    "    def next(self, batch_size):\n",
    "\n",
    "        if self.batch_id == len(self.data):\n",
    "            self.batch_id = 0\n",
    "            \n",
    "        batch_data = (self.data[self.batch_id:min(self.batch_id + batch_size, len(self.data))])\n",
    "        \n",
    "        batch_labels = (self.labels[self.batch_id:min(self.batch_id + batch_size, len(self.data))])\n",
    "        \n",
    "        batch_seqlen = (self.seqlen[self.batch_id:min(self.batch_id + batch_size, len(self.data))])\n",
    "        \n",
    "        self.batch_id = min(self.batch_id + batch_size, len(self.data))\n",
    "        \n",
    "        return batch_data, batch_labels, batch_seqlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========\n",
    "#   MODEL\n",
    "# ==========\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "training_steps = 10000\n",
    "batch_size = 128\n",
    "display_step = 1000\n",
    "\n",
    "# Network Parameters\n",
    "seq_max_len = maxlen # Sequence max length\n",
    "word_features = 25\n",
    "n_hidden_1 = 20 # 1st layer number of neurons\n",
    "n_hidden_2 = 20 # 2nd layer number of neurons\n",
    "n_classes = 2 # linear sequence or notstate_size = 1#20\n",
    "\n",
    "#trainset = SequenceData(n_samples = N_train,\n",
    "#                           max_seq_len = seq_max_len,\n",
    "#                           x_list = tweetList_encoded_train,\n",
    "#                           t_list = labelList_train)\n",
    "\n",
    "#testset = SequenceData(n_samples = N_valid,\n",
    "#                          max_seq_len = seq_max_len,\n",
    "#                          x_list = tweetList_encoded_valid,\n",
    "#                          t_list = labelList_valid)\n",
    "\n",
    "trainset = SequenceData(n_samples = N,\n",
    "                           max_seq_len = seq_max_len,\n",
    "                           x_list = tweetList_encoded,\n",
    "                           t_list = labelList)\n",
    "\n",
    "testset = SequenceData(n_samples = N_test,\n",
    "                       max_seq_len = seq_max_len,\n",
    "                       x_list = tweetList_test_encoded,\n",
    "                       t_list = [])\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(\"int32\", [None, seq_max_len])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# A placeholder for indicating each sequence length\n",
    "seqlen = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "# Embedding matrix\n",
    "embeddings = tf.Variable(tf.random_uniform([M, word_features] , -1.0 , 1.0 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_net_dense(x):\n",
    "    layer_1 = tf.layers.dense(x, n_hidden_1,\n",
    "                              kernel_regularizer = tf.contrib.layers.l2_regularizer(0.001),\n",
    "                              kernel_initializer = tf.truncated_normal_initializer(stddev=sqrt(2.0/n_hidden_1)))\n",
    "    layer_2 = tf.layers.dense(layer_1, n_hidden_2,\n",
    "                              kernel_regularizer = tf.contrib.layers.l2_regularizer(0.001),\n",
    "                              kernel_initializer = tf.truncated_normal_initializer(stddev=sqrt(2.0/n_hidden_2)))\n",
    "    out_layer = tf.layers.dense(layer_2, n_classes,\n",
    "                                kernel_regularizer = tf.contrib.layers.l2_regularizer(0.001),\n",
    "                                kernel_initializer = tf.truncated_normal_initializer(stddev=np.sqrt(2.0/n_classes)))\n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamicRNN(x, seqlen, embeddings):\n",
    "    #\n",
    "    x = tf.nn.embedding_lookup(embeddings, x)\n",
    "    \n",
    "    #\n",
    "    x = tf.unstack(x, seq_max_len, 1)\n",
    "    \n",
    "    #\n",
    "    lstm_cell = tf.contrib.rnn.BasicLSTMCell(word_features)\n",
    "    \n",
    "    #\n",
    "    outputs = tf.contrib.rnn.static_rnn(lstm_cell, x, dtype=tf.float32, sequence_length=seqlen)[0]\n",
    "    \n",
    "    #\n",
    "    outputs = tf.nn.dropout(outputs, keep_prob=0.6)\n",
    "\n",
    "\n",
    "    #\n",
    "    outputs = tf.stack(outputs)\n",
    "    outputs = tf.transpose(outputs, [1, 0, 2])\n",
    "    \n",
    "    #\n",
    "    batch_size = tf.shape(outputs)[0]\n",
    "    index = tf.range(batch_size) * seq_max_len + (seqlen - 1)\n",
    "    outputs = tf.gather(tf.reshape(outputs, [-1, word_features]), index)\n",
    "    \n",
    "    #\n",
    "    #return neural_net(outputs, weights, biases)\n",
    "    return neural_net_dense(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:96: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "pred = dynamicRNN(x, seqlen, embeddings)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Evaluate model\n",
    "class_pred = tf.argmax(pred, axis=1)\n",
    "prob_pred = tf.nn.softmax(pred)\n",
    "\n",
    "correct_pred = tf.equal(tf.argmax(pred,axis=1), tf.argmax(y,axis=1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# saver\n",
    "saver = tf.train.Saver(max_to_keep=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1, Minibatch Loss= 2.846292, Training Accuracy= 0.63281\n",
      "Step 1000, Minibatch Loss= 0.459150, Training Accuracy= 0.78125\n",
      "Step 2000, Minibatch Loss= 0.611614, Training Accuracy= 0.64844\n",
      "Step 3000, Minibatch Loss= 0.053200, Training Accuracy= 1.00000\n",
      "Step 4000, Minibatch Loss= 0.554319, Training Accuracy= 0.75781\n",
      "Step 5000, Minibatch Loss= 0.427462, Training Accuracy= 0.82031\n",
      "Step 6000, Minibatch Loss= 0.044267, Training Accuracy= 0.99219\n",
      "Step 7000, Minibatch Loss= 0.339143, Training Accuracy= 0.85938\n",
      "Step 8000, Minibatch Loss= 0.271917, Training Accuracy= 0.82812\n",
      "Step 9000, Minibatch Loss= 0.070379, Training Accuracy= 0.99219\n",
      "Step 10000, Minibatch Loss= 0.255168, Training Accuracy= 0.92188\n",
      "Optimization Finished!\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-c1dcf7d558cb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[1;31m# Calculate output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m     \u001b[0mtest_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprob_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseqlen\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtest_seqlen\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32md:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    887\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 889\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    890\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1087\u001b[0m             \u001b[0mfeed_handles\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msubfeed_val\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1088\u001b[0m           \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1089\u001b[1;33m             \u001b[0mnp_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msubfeed_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1090\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1091\u001b[0m           if (not is_tensor_handle_feed and\n",
      "\u001b[1;32md:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\numpy\\core\\numeric.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m    529\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m     \"\"\"\n\u001b[1;32m--> 531\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    532\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    533\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "    \n",
    "    #saver.restore(sess, \"./Model/model.ckpt\")\n",
    "\n",
    "    for step in range(1, training_steps+1):\n",
    "        batch_x, batch_y, batch_seqlen = trainset.next(batch_size)\n",
    "        # Run optimization op (backprop)\n",
    "        sess.run(optimizer, feed_dict={x: batch_x, \n",
    "                                       y: batch_y, \n",
    "                                       seqlen: batch_seqlen})\n",
    "        if step % display_step == 0 or step == 1:\n",
    "            # Calculate batch accuracy & loss\n",
    "            acc, loss = sess.run([accuracy, cost], feed_dict={x: batch_x, \n",
    "                                                              y: batch_y, \n",
    "                                                              seqlen: batch_seqlen})\n",
    "            \n",
    "            print(\"Step \" + str(step) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.5f}\".format(acc))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "    \n",
    "    # Save Model\n",
    "    saver.save(sess, \"Model/model.ckpt\")\n",
    "    \n",
    "    # Get Test Data\n",
    "    test_data = testset.data\n",
    "    test_label = testset.labels\n",
    "    test_seqlen = testset.seqlen\n",
    "\n",
    "    # Calculate accuracy\n",
    "    #print(\"Testing Accuracy:\", sess.run(accuracy, feed_dict={x: test_data, \n",
    "    #                                                         y: test_label, \n",
    "    #                                                         seqlen: test_seqlen}))\n",
    "    \n",
    "    # Calculate loss\n",
    "    #print(\"Testing Loss:\", sess.run(cost, feed_dict={x: test_data, \n",
    "    #                                                 y: test_label, \n",
    "    #                                                 seqlen: test_seqlen}))\n",
    "    \n",
    "    # Calculate output\n",
    "    test_result = sess.run(prob_pred, feed_dict={x: test_data, seqlen: test_seqlen})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    saver.restore(sess, \"Model/model.ckpt\")\n",
    " \n",
    "    # Get Test Data\n",
    "    test_data = testset.data\n",
    "    test_seqlen = testset.seqlen\n",
    "\n",
    "    # Calculate output\n",
    "    test_result = sess.run(prob_pred, feed_dict={x: test_data, seqlen: test_seqlen})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('test_result.csv', 'w', newline='') as csvFile:\n",
    "    writer = csv.writer(csvFile)\n",
    "    fileheader = [\"id\", \"realDonaldTrump\", \"HillaryClinton\"]\n",
    "    writer.writerow(fileheader)\n",
    "    for i in range(len(test_result)):\n",
    "        #onerow = [i, tweetList_test[i],test_result[i][1], test_result[i][0]]\n",
    "        writer.writerow([i, test_result[i][1], test_result[i][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
